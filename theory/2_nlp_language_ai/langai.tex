\section{Language Artificial Intelligence} % (fold)
\label{sec:language_ai}
%
\begin{frame}[t,fragile] \frametitle{Language AI}
    \framesubtitle{\ldots o NLP}
	{\small
	    \begin{minipage}[t]{\textwidth}
	    	\begin{itemize}[leftmargin=10pt,align=right]
				\onslide<1->\item[\alert{\faHandORight}] Sottocampo dell'AI dedicato allo sviluppo di tecnologie per il linguaggio umano
				\begin{itemize}[leftmargin=10pt,align=right]
					\item[\alert{\faHandORight}] Comprensione
					\item[\alert{\faHandORight}] Elaborazione
					\item[\alert{\faHandORight}] Generazione
				\end{itemize}
				\onslide<2->\item[\alert{\faHandORight}] Utilizzato intercambiabilmente con \alert{Natural Language Processing} (NLP)
				\onslide<3->\item[\alert{\faExclamationTriangle}] \alert{Trasversale} rispetto alla classificazione canonica
			\end{itemize}
	    \end{minipage}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Le origini: ELIZA (1966)}
    \framesubtitle{Il primo \textit{chatbot} della storia}
	{\small
	    \begin{minipage}[t]{\textwidth}
	    	\begin{itemize}[leftmargin=10pt,align=right]
				\onslide<1->\item[\alert{\faHandORight}] Sviluppato da \alert{Joseph Weizenbaum} al MIT
				\onslide<2->\item[\alert{\faHandORight}] Simulava una conversazione con uno \alert{psicoterapeuta} rogersiano
				\onslide<3->\item[\alert{\faHandORight}] Utilizzava semplici \alert{\textit{pattern matching}} e regole di sostituzione
				\onslide<4->\item[\alert{\faHandORight}] Dimostrava quanto facilmente le persone potessero essere \alert{ingannate} da un programma semplice
				\onslide<5->\item[\alert{\faHandORight}] Primo esempio di \alert{illusione di comprensione} da parte di una macchina
			\end{itemize}
        \end{minipage}
		\vfill
		\onslide<1->
		\begin{center}
			\href{https://www.masswerk.at/elizabot/eliza.html}{\faLink\ Implementazione ELIZA\ldots}
		\end{center}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{ELIZA: come funzionava}
	{\small
		\framesubtitle{Pattern matching e trasformazioni di template}
		\begin{itemize}[leftmargin=10pt,align=right]
			\item[\alert{\faHandORight}] \alert{Trasformazioni grammaticali:} regole sintattiche applicate all'\textit{input} utente
		\end{itemize}
		\vspace*{.3cm}
		\hspace*{4cm}
		\begin{shellcodeblock}{Regole di trasformazione ELIZA}
        	\begin{minted}{shell-session}
''I am'' -> ''you are''
''my'' -> ''your''
''me'' -> ''you''
...
        	\end{minted}
    	\end{shellcodeblock}
		\hspace*{4cm}
		\begin{minipage}[t]{.6\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{\alert{User:} ``\alert{I am} feeling sad today''\\
			\alert{ELIZA (thinking):} --- \alert{you are} feeling sad today---}}{\textbf{ELIZA, elaborazione, 1966}}
		\end{minipage}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{ELIZA: come funzionava}
	{\small
		\framesubtitle{Pattern matching e trasformazioni di template}
		\begin{itemize}[leftmargin=10pt,align=right]
			\item[\alert{\faHandORight}] \alert{\textit{Template} di risposta:} frasi predefinite con \textit{slot} per le sostituzioni
		\end{itemize}
		\vspace*{.3cm}
		\hspace*{4cm}
		\begin{shellcodeblock}{Esempi di \textit{template} ELIZA}
        	\begin{minted}{shell-session}
''Tell me more about ___''
''What else comes to mind when ___?''
''Why ___?''
...
        	\end{minted}
    	\end{shellcodeblock}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{ELIZA: come funzionava}
	{\small
		\onslide<1->
		\framesubtitle{Pattern matching e trasformazioni di template}
		\begin{itemize}[leftmargin=10pt,align=right]
			\item[\alert{\faHandORight}] \alert{Regole di \textit{pattern matching}:} riconoscimento di parole chiave nell'\textit{input} e scelta fra possibili \textit{pattern} correlati
		\end{itemize}
		\vspace*{.3cm}
		\begin{shellcodeblock}{Esempi di \textit{pattern matching} ELIZA}
        	\begin{minted}{shell-session}
''I think about ___'' -> ''Tell me more about ___''
''I am thinking of ___'' -> ''What else comes to mind when you think of ___?''
''I am ___'' -> ''Why do you think ___?''
...
        	\end{minted}
    	\end{shellcodeblock}
		\hspace*{4cm}
		\begin{minipage}[t]{.6\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{\alert{User:} ``\alert{I am} feeling sad today''\\
			\alert{ELIZA:} ``Why do you think you are feeling sad today?''}}{\textbf{ELIZA, esempio di risposta, 1966}}
		\end{minipage}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{ELIZA: come funzionava}
	\framesubtitle{Pattern matching e trasformazioni di template}
	{\small
		\begin{itemize}[leftmargin=10pt,align=right]
			\item[\alert{\faHandORight}] \alert{Strategia di \textit{fallback}:} formule generiche quando non trova \textit{pattern}
		\end{itemize}
		\vspace*{.3cm}
		\hspace*{4cm}
		\begin{minipage}[t]{.6\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{\alert{User:} ``\alert{My} whole world is falling apart.''\\
			\alert{ELIZA:} ``I see.''\\
			(oppure)\\
			\alert{ELIZA:} ``Can you elaborate on that?''\\
			(oppure)\\
			\alert{ELIZA:} ``I see.''\\
			(oppure)\\
			\alert{ELIZA:} ``What does that suggest to you?''\\
			(oppure)\\
			\alert{ELIZA:} ``Please go on.''\\
			(oppure)\\
			\alert{ELIZA:} ``How does that make you feel?''
			}}{\textbf{ELIZA, esempio di risposta, 1966}}
		\end{minipage}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Bag-of-Words}
    \framesubtitle{Rappresentazione testo non strutturato}
	{\small
		\begin{minipage}[t]{\textwidth}
	    	\begin{itemize}[leftmargin=10pt,align=right]
				\onslide<1->\item[\alert{\faHandORight}] Menzionato per la prima volta negli anni '50, popolare negli anni 2000
				\onslide<2->\item[\alert{\faHandORight}] Metodo per rappresentare il testo \alert{non strutturato} in formato numerico
				\onslide<3->\item[\alert{\faHandORight}] Il \alert{linguaggio è complicato} per i calcolatori
				\begin{itemize}[leftmargin=10pt,align=right]
					\item[\alert{\faHandORight}] Il testo perde significato quando rappresentato da 0 e 1
				\end{itemize}
				\onslide<4->\item[\alert{\faHandORight}] \alert{Focus principale:} rappresentare il linguaggio in \alert{modo strutturato} per l'uso da parte dei calcolatori
			\end{itemize}
        \end{minipage}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Bag-of-Words: come funziona}
	{\footnotesize
		\onslide<1->
		\framesubtitle{Processo di tokenizzazione e creazione del vocabolario}
		\begin{enumerate}[leftmargin=10pt,align=right]
			\onslide<1->{\item[\alertedcircled{1}] \alert{Tokenizzazione:} processo di divisione delle frasi in parole individuali o sotto-parole (\alert{\textit{token}})
			\begin{itemize}[leftmargin=10pt,align=right]
				\item[\alert{\faHandORight}] Rispetto a un \alert{delimitatore} (specifico o \textit{wildcard})
				\item[\alert{\faHandORight}] Definendo una \alert{\textit{blacklist}}
			\end{itemize}
			\onslide<2->{\item[\alertedcircled{2}] \alert{Creazione del vocabolario:} estrazione di entità uniche dai \textit{token}
			\begin{itemize}[leftmargin=10pt,align=right]
				\item[\alert{\faHandORight}] Applicando estrazione della radice (\alert{\textit{stemming}})
				\item[\alert{\faExclamationTriangle}] Vettore \alert{ordinato} di radici
			\end{itemize}}
			\onslide<3->{\item[\alertedcircled{3}] \alert{Conteggio delle parole:} rappresentazione numerica basata sulla \alert{frequenza}}
		\end{itemize}
		\hspace*{1cm}
		\only<1|handout:1>{
		\begin{minipage}[t]{.9\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{0pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{\alert{F1:} ``Che\,\Vtextvisiblespace[1em]\,bel\,\Vtextvisiblespace\,cane\,\Vtextvisiblespace[1cm]\,che\,\Vtextvisiblespace[.5em]\,hai\,!''\\
			\alert{F2:} ``Ma\,\Vtextvisiblespace[.3cm]\,il\,\Vtextvisiblespace[1cm]\,mio\,\Vtextvisiblespace[.1cm]\,gatto\,\Vtextvisiblespace[1em]\,è\,\Vtextvisiblespace[.5em]\,più\,\Vtextvisiblespace[1cm]\,bello''\\
		{\tiny
		\begin{table}
			\setlength{\tabcolsep}{4pt}
			\renewcommand{\arraystretch}{1}
			\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
				%\toprule
				\multicolumn{12}{c}{\textbf{Token}}\\
				\midrule
				che & bel & cane & che & hai & ma & il & mio & gatto & è & più & bello\\
				\midrule
				%\bottomrule
			\end{tabular}
		\end{table}
		}
			}}{\textbf{Tokenizzazione}}

		\end{minipage}
		}
		\only<2|handout:2>{
		\begin{minipage}[t]{.9\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{0pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{\alert{F1:} ``Che\,\Vtextvisiblespace[1em]\,bel\,\Vtextvisiblespace\,cane\,\Vtextvisiblespace[1cm]\,che\,\Vtextvisiblespace[.5em]\,hai\,!''\\
			\alert{F2:} ``Ma\,\Vtextvisiblespace[.3cm]\,il\,\Vtextvisiblespace[1cm]\,mio\,\Vtextvisiblespace[.1cm]\,gatto\,\Vtextvisiblespace[1em]\,è\,\Vtextvisiblespace[.5em]\,più\,\Vtextvisiblespace[1cm]\,bello''\\
		{\tiny
		\begin{table}
			\setlength{\tabcolsep}{4pt}
			\renewcommand{\arraystretch}{1}
			\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
				%\toprule
				\multicolumn{11}{c}{\textbf{Vocabolario}}\\
				\midrule
				che & bel & cane & avere & ma & il & mio & gatto & essere & più & bello\\
				\midrule
				%\bottomrule
			\end{tabular}
		\end{table}
		}
		}}{\textbf{Creazione del vocabolario}}
		\end{minipage}
		}
		\only<3|handout:3>{
		\begin{minipage}[t]{.9\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{0pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{\alert{F1:} ``Che\,\Vtextvisiblespace[1em]\,bel\,\Vtextvisiblespace\,cane\,\Vtextvisiblespace[1cm]\,che\,\Vtextvisiblespace[.5em]\,hai\,!''\\
			\alert{F2:} ``Ma\,\Vtextvisiblespace[.3cm]\,il\,\Vtextvisiblespace[1cm]\,mio\,\Vtextvisiblespace[.1cm]\,gatto\,\Vtextvisiblespace[1em]\,è\,\Vtextvisiblespace[.5em]\,più\,\Vtextvisiblespace[1cm]\,bello''\\
		{\tiny
		\begin{table}
			\setlength{\tabcolsep}{4pt}
			\renewcommand{\arraystretch}{1}
			\begin{tabular}{cc|c|c|c|c|c|c|c|c|c|c}
								& \multicolumn{11}{c}{\textbf{Vocabolario}}\\
				\cmidrule{2-12}
				        & che & bel & cane & avere & ma & il & mio & gatto & essere & più & bello\\
				\cmidrule{2-12}
				\alert{F1} & 2   & 1   & 1    & 1     & 0  & 0  & 0   & 0     & 0      & 0   & 0    \\
				\alert{F2} & 0   & 0   & 0    & 0     & 1  & 1  & 1   & 1     & 1      & 1   & 1    \\
				\midrule
				%\bottomrule
			\end{tabular}
		\end{table}
		}
		}}{\textbf{Vettori \textit{bag-of-words}}}
		\end{minipage}
		}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Limitazioni del Bag-of-Words}
	{\small
		\onslide<1->
		\framesubtitle{Perché servivano rappresentazioni migliori}
		\begin{itemize}[leftmargin=10pt,align=right]
			\onslide<2->\item[\alert{\faHandORight}] \alert{Perdita dell'ordine:} "The dog bit the man" = "The man bit the dog"
			\onslide<3->\item[\alert{\faHandORight}] \alert{Mancanza di semantica:} Non cattura il significato delle parole
			\onslide<4->\item[\alert{\faHandORight}] \alert{Alta dimensionalità:} Vocabolari enormi con molti zeri
			\onslide<5->\item[\alert{\faHandORight}] \alert{Nessuna generalizzazione:} i sinonimi sono trattati come elementi \alert{totalmente separati}
		\end{itemize}
		\vspace*{.3cm}
		\only<2|handout:1>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{Due frasi con significati opposti:\\
			"The dog bit the man" -> (1, 1, 1, 1, 1)\\
			"The man bit the dog" -> (1, 1, 1, 1, 1)\\
			Identiche rappresentazioni per significati diversi!}}{\textbf{Problema dell'ordine delle parole}}
		\end{minipage}
		}
		\only<3|handout:2>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{Parole correlate semanticamente:\\
			"king" e "queen" (concetti correlati)\\
			"happy" e "joyful" (sinonimi)\\
			"dog" e "puppy" (relazione iperonimo-iponimo)\\
			Nel bag-of-words sono tutte completamente diverse}}{\textbf{Mancanza di relazioni semantiche}}
		\end{minipage}
		}
		\only<4|handout:3>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{Esempio con vocabolario di Wikipedia:\\
			- Vocabolario: ~6 milioni di parole\\
			- Vettore per ogni documento: 6 milioni di dimensioni\\
			- La maggior parte dei valori è 0 (matrice sparsa)\\
			Computazionalmente inefficiente!}}{\textbf{Problema della dimensionalità}}
		\end{minipage}
		}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Evoluzione: Word Embeddings (2013)}
	{\scriptsize
		\onslide<1->
            \framesubtitle{Word2Vec e la rivoluzione delle rappresentazioni dense}
            \vspace*{-15pt}
	    	\begin{minipage}[t]{\textwidth}
				\begin{minipage}[t]{0.6\textwidth}
	    			\begin{itemize}[leftmargin=10pt,align=right]
						\onslide<2->\item[\alert{\faHandORight}] \alert{Word2Vec:} primo tentativo di successo per catturare il significato del testo negli embeddings
						\onslide<3->\item[\alert{\faHandORight}] \alert{Embeddings:} rappresentazioni vettoriali di dati che tentano di catturarne il significato
						\onslide<4->\item[\alert{\faHandORight}] Addestrato su \alert{enormi quantità} di dati testuali (intera Wikipedia)
						\onslide<5->\item[\alert{\faHandORight}] Utilizza \alert{reti neurali} per generare rappresentazioni semantiche
					\end{itemize}
            	\end{minipage}
            	%
	    	\end{minipage}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Word2Vec: Come Funziona}
	{\small
		\onslide<1->
		\framesubtitle{Apprendimento delle relazioni tra parole}
		\begin{itemize}[leftmargin=10pt,align=right]
			\onslide<2->\item[\alert{\faHandORight}] \alert{Principio fondamentale:} parole che appaiono in contesti simili hanno significati simili
			\onslide<3->\item[\alert{\faHandORight}] \alert{Addestramento:} predire se due parole sono vicine in una frase
			\onslide<4->\item[\alert{\faHandORight}] \alert{Risultato:} parole con significati simili hanno embeddings vicini nello spazio
		\end{itemize}
		\vspace*{.3cm}
		\only<2|handout:1>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{Ipotesi distributiva:\\
			"You shall know a word by the company it keeps"\\
			Esempi di contesti:\\
			"The \_\_\_ is barking" -> probabilmente "dog"\\
			"I love my pet \_\_\_" -> "dog", "cat", "bird"}}{\textbf{Principio alla base di Word2Vec}}
		\end{minipage}
		}
		\only<3|handout:2>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{Processo di addestramento:\\
			1. Assegna a ogni parola un vettore casuale (50 dimensioni)\\
			2. Prendi coppie di parole dal testo\\
			3. La rete predice: sono vicine in una frase?\\
			4. Aggiorna i vettori in base alla correttezza}}{\textbf{Training di Word2Vec}}
		\end{minipage}
		}
		\only<4|handout:3>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{Risultati sorprendenti:\\
			king - man + woman ≈ queen\\
			Paris - France + Italy ≈ Rome\\
			walking - walk + swim ≈ swimming\\
			Gli embeddings catturano relazioni semantiche!}}{\textbf{Proprietà emergenti degli embeddings}}
		\end{minipage}
		}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Dalle Reti Ricorrenti ai Transformer (2017)}
	{\scriptsize
		\onslide<1->
            \framesubtitle{L'evoluzione verso l'attenzione}
            \vspace*{-15pt}
	    	\begin{minipage}[t]{\textwidth}
				\begin{minipage}[t]{0.6\textwidth}
	    			\begin{itemize}[leftmargin=10pt,align=right]
						\onslide<2->\item[\alert{\faHandORight}] \alert{Problema delle RNN:} processamento sequenziale, difficoltà con sequenze lunghe
						\onslide<3->\item[\alert{\faHandORight}] \alert{Meccanismo di attenzione:} permette al modello di "prestare attenzione" a parti rilevanti dell'input
						\onslide<4->\item[\alert{\faHandORight}] \alert{Paper "Attention Is All You Need":} introduce l'architettura Transformer
						\onslide<5->\item[\alert{\faHandORight}] \alert{Vantaggi:} parallelizzazione, migliori performance, base per i moderni LLM
					\end{itemize}
            	\end{minipage}
            	%
	    	\end{minipage}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Transformer: Rivoluzione dell'Attenzione}
	{\small
		\onslide<1->
		\framesubtitle{Come l'attenzione ha cambiato tutto}
		\begin{itemize}[leftmargin=10pt,align=right]
			\onslide<2->\item[\alert{\faHandORight}] \alert{Self-attention:} ogni parola può "prestare attenzione" a ogni altra parola nella sequenza
			\onslide<3->\item[\alert{\faHandORight}] \alert{Parallelizzazione:} non più processamento sequenziale, ma simultaneo
			\onslide<4->\item[\alert{\faHandORight}] \alert{Gestione del contesto:} cattura relazioni a lungo raggio tra parole
		\end{itemize}
		\vspace*{.3cm}
		\only<2|handout:1>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{Esempio di self-attention:\\
			Frase: "The animal didn't cross the street because it was too tired"\\
			La parola "it" presta alta attenzione a "animal"\\
			Il modello comprende il riferimento pronominale}}{\textbf{Self-attention in azione}}
		\end{minipage}
		}
		\only<3|handout:2>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{RNN: parola1 -> parola2 -> parola3 -> ... (sequenziale)\\
			Transformer: tutte le parole processate simultaneamente\\
			Risultato: training 100x più veloce su GPU}}{\textbf{Vantaggio della parallelizzazione}}
		\end{minipage}
		}
		\only<4|handout:3>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{Gestione di dipendenze a lungo raggio:\\
			"Maria, che aveva studiato linguistica per anni, \_\_\_ esperta"\\
			Il Transformer collega "Maria" con il verbo finale\\
			anche attraverso la lunga frase subordinata}}{\textbf{Cattura del contesto a lungo raggio}}
		\end{minipage}
		}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{BERT vs GPT: Due Paradigmi}
	{\scriptsize
		\onslide<1->
            \framesubtitle{Modelli di rappresentazione vs modelli generativi}
            \vspace*{-10pt}
	    	\begin{minipage}[t]{\textwidth}
				\begin{minipage}[t]{0.6\textwidth}
	    			\begin{itemize}[leftmargin=10pt,align=right]
						\onslide<2->\item[\alert{\faHandORight}] \alert{BERT (2018):} Bidirectional Encoder Representations from Transformers
						\begin{itemize}[leftmargin=10pt,align=right]
							\item[\alert{\faHandORight}] Architettura encoder-only
							\item[\alert{\faHandORight}] Masked Language Modeling
							\item[\alert{\faHandORight}] Eccelle nella comprensione
						\end{itemize}
						\onslide<3->\item[\alert{\faHandORight}] \alert{GPT (2018):} Generative Pretrained Transformer
						\begin{itemize}[leftmargin=10pt,align=right]
							\item[\alert{\faHandORight}] Architettura decoder-only
							\item[\alert{\faHandORight}] Next Token Prediction
							\item[\alert{\faHandORight}] Eccelle nella generazione
						\end{itemize}
					\end{itemize}
            	\end{minipage}
            	%
	    	\end{minipage}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Introduzione alla Language AI}
	{\scriptsize
		\onslide<1->
            \framesubtitle{Dall'elaborazione del linguaggio naturale ai modelli linguistici di grandi dimensioni}
            \vspace*{-15pt}
	    	\begin{minipage}[t]{\textwidth}
				\begin{minipage}[t]{0.6\textwidth}
	    			\begin{itemize}[leftmargin=10pt,align=right]
						\onslide<2->\item[\alert{\faHandORight}] \alert{GPT-2 (2019):} 1.5 miliardi di parametri, "troppo pericoloso da rilasciare"
						\onslide<3->\item[\alert{\faHandORight}] \alert{GPT-3 (2020):} 175 miliardi di parametri, capacità emergenti
						\onslide<4->\item[\alert{\faHandORight}] \alert{ChatGPT (2022):} fine-tuning con feedback umano (RLHF)
						\onslide<5->\item[\alert{\faHandORight}] \alert{GPT-4 e successivi:} modelli multimodali, ragionamento avanzato
					\end{itemize}
            	\end{minipage}
	    	\end{minipage}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Capacità Emergenti dei LLM}
	{\small
		\onslide<1->
		\framesubtitle{Abilità che emergono con la scala}
		\begin{itemize}[leftmargin=10pt,align=right]
			\onslide<2->\item[\alert{\faHandORight}] \alert{In-context Learning:} apprendimento da pochi esempi nel prompt
			\onslide<3->\item[\alert{\faHandORight}] \alert{Chain-of-Thought:} ragionamento step-by-step
			\onslide<4->\item[\alert{\faHandORight}] \alert{Code Generation:} generazione di codice da descrizioni
			\onslide<5->\item[\alert{\faHandORight}] \alert{Multimodalità:} comprensione di testo, immagini, audio
		\end{itemize}
		\vspace*{.3cm}
		\only<2|handout:1>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{Esempi di in-context learning:\\
			"Translate to French: Hello -> Bonjour\\
			Translate to French: Goodbye -> Au revoir\\
			Translate to French: Thank you -> Merci"\\
			Il modello impara il pattern senza training!}}{\textbf{Few-shot learning con i LLM}}
		\end{minipage}
		}
		\only<3|handout:2>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{Chain-of-Thought prompting:\\
			"Risolvi step by step: Se ho 15 mele e ne mangio 3, poi ne compro il doppio di quelle rimanenti, quante mele ho?"\\
			Risposta: "Prima calcolo: 15 - 3 = 12 mele rimaste\\
			Poi: 12 × 2 = 24 mele comprate\\
			Totale: 12 + 24 = 36 mele"}}{\textbf{Ragionamento esplicito}}
		\end{minipage}
		}
		\only<4|handout:3>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{Prompt: "Scrivi una funzione Python che calcola il fattoriale"\\
			Output:\\
			def factorial(n):\\
			\quad if n == 0 or n == 1:\\
			\quad\quad return 1\\
			\quad return n * factorial(n-1)}}{\textbf{Generazione di codice}}
		\end{minipage}
		}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Architetture dei Moderni LLM}
	{\scriptsize
		\onslide<1->
            \framesubtitle{Decoder-only Transformers dominano}
            \vspace*{-10pt}
	    	\begin{minipage}[t]{\textwidth}
				\begin{minipage}[t]{0.6\textwidth}
	    			\begin{itemize}[leftmargin=10pt,align=right]
						\onslide<2->\item[\alert{\faHandORight}] \alert{Architettura dominante:} Decoder-only Transformers
						\onslide<3->\item[\alert{\faHandORight}] \alert{Autoregressive Generation:} generazione token per token
						\onslide<4->\item[\alert{\faHandORight}] \alert{Scaling Laws:} performance migliora con più parametri e dati
						\onslide<5->\item[\alert{\faHandORight}] \alert{Emergenza:} nuove capacità appaiono improvvisamente oltre certe scale
					\end{itemize}
            	\end{minipage}
            	%
	    	\end{minipage}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Tipi di Embeddings nei LLM}
	{\small
		\onslide<1->
		\framesubtitle{Dall'input agli embeddings contestuali}
		\begin{itemize}[leftmargin=10pt,align=right]
			\onslide<2->\item[\alert{\faHandORight}] \alert{Token Embeddings:} rappresentazioni delle singole parole/subword
			\onslide<3->\item[\alert{\faHandORight}] \alert{Positional Embeddings:} informazione sulla posizione nella sequenza
			\onslide<4->\item[\alert{\faHandORight}] \alert{Contextual Embeddings:} rappresentazioni che cambiano in base al contesto
		\end{itemize}
		\vspace*{.3cm}
		\only<2|handout:1>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{Tokenizzazione subword (BPE):\\
			"GPT-4 is amazing" ->\\
			("G", "PT", "-4", " is", " amazing")\\
			Ogni token ha il suo embedding vettoriale}}{\textbf{Da testo a token embeddings}}
		\end{minipage}
		}
		\only<3|handout:2>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{I Transformers non hanno concetto di ordine\\
			Positional embeddings aggiungono informazione:\\
			Posizione 1: (0.1, -0.5, 0.8, ...)\\
			Posizione 2: (0.3, -0.1, 0.2, ...)\\
			...\\
			Sommati agli embeddings dei token}}{\textbf{Encoding della posizione}}
		\end{minipage}
		}
		\only<4|handout:3>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{Stesso token, diversi contesti:\\
			"I went to the bank" (istituto finanziario)\\
			"I sat by the river bank" (sponda del fiume)\\
			L'embedding di "bank" cambia in base al contesto\\
			grazie ai layer di attenzione}}{\textbf{Embeddings contestuali}}
		\end{minipage}
		}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Training e Fine-tuning dei LLM}
	{\scriptsize
		\onslide<1->
            \framesubtitle{Dal pre-training all'allineamento}
            \vspace*{-10pt}
	    	\begin{minipage}[t]{\textwidth}
				\begin{minipage}[t]{0.6\textwidth}
	    			\begin{itemize}[leftmargin=10pt,align=right]
						\onslide<2->\item[\alert{\faHandORight}] \alert{Pre-training:} apprendimento auto-supervisionato su enormi corpus di testo
						\onslide<3->\item[\alert{\faHandORight}] \alert{Instruction Tuning:} fine-tuning per seguire istruzioni
						\onslide<4->\item[\alert{\faHandORight}] \alert{RLHF:} Reinforcement Learning from Human Feedback
						\onslide<5->\item[\alert{\faHandORight}] \alert{Constitutional AI:} addestramento con principi etici
					\end{itemize}
            	\end{minipage}
            	%
	    	\end{minipage}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Applicazioni dei LLM}
	{\small
		\onslide<1->
		\framesubtitle{Dai chatbot agli assistenti specializzati}
		\begin{itemize}[leftmargin=10pt,align=right]
			\onslide<2->\item[\alert{\faHandORight}] \alert{Assistenti conversazionali:} ChatGPT, Claude, Bard
			\onslide<3->\item[\alert{\faHandORight}] \alert{Generazione di codice:} GitHub Copilot, CodeT5
			\onslide<4->\item[\alert{\faHandORight}] \alert{Ricerca e RAG:} Retrieval-Augmented Generation
			\onslide<5->\item[\alert{\faHandORight}] \alert{Agenti autonomi:} AutoGPT, LangChain
		\end{itemize}
		\vspace*{.3cm}
		\only<3|handout:1>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{// Scrivi una funzione per ordinare una lista\\
			function quickSort(arr) \{\\
			\quad if (arr.length <= 1) return arr;\\
			\quad const pivot = arr[0];\\
			\quad const left = arr.slice(1).filter(x => x < pivot);\\
			\quad const right = arr.slice(1).filter(x => x >= pivot);\\
			\quad return [...quickSort(left), pivot, ...quickSort(right)];\\
			\}}}{\textbf{Generazione automatica di codice}}
		\end{minipage}
		}
		\only<4|handout:2>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{RAG (Retrieval-Augmented Generation):\\
			1. Utente fa una domanda\\
			2. Sistema cerca informazioni rilevanti in un database\\
			3. LLM genera risposta usando le informazioni trovate\\
			4. Risposta accurata e aggiornata\\
			Combina ricerca e generazione!}}{\textbf{Ricerca potenziata dalla generazione}}
		\end{minipage}
		}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Sfide e Limitazioni Attuali}
	{\small
		\onslide<1->
		\framesubtitle{Problemi ancora aperti}
		\begin{itemize}[leftmargin=10pt,align=right]
			\onslide<2->\item[\alert{\faHandORight}] \alert{Allucinazioni:} generazione di informazioni false o inventate
			\onslide<3->\item[\alert{\faHandORight}] \alert{Bias:} pregiudizi presenti nei dati di training
			\onslide<4->\item[\alert{\faHandORight}] \alert{Interpretabilità:} difficoltà nel capire come ragionano
			\onslide<5->\item[\alert{\faHandORight}] \alert{Costi computazionali:} enormi risorse necessarie per training e inferenza
		\end{itemize}
		\vspace*{.3cm}
		\only<2|handout:1>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{Esempi di allucinazioni:\\
			"Chi ha vinto il Nobel per la Fisica nel 2025?"\\
			LLM: "Mario Rossi per i suoi studi sui buchi neri"\\
			Genera risposte plausibili ma completamente false}}{\textbf{Il problema delle allucinazioni}}
		\end{minipage}
		}
		\only<5|handout:2>{
		\begin{minipage}[t]{\textwidth}
			\renewcommand{\epigraphsize}{\scriptsize}
			\setlength{\afterepigraphskip}{0pt}
			\setlength{\beforeepigraphskip}{5pt}
			\setlength{\epigraphwidth}{0.9\textwidth}
			\epigraph{\textit{Costi di GPT-4:\\
			- Training: >100 milioni di dollari\\
			- 25.000 GPU A100 per mesi\\
			- Inferenza: migliaia di GPU in parallelo\\
			- Consumo energetico: paragonabile a una piccola città\\
			Sostenibilità ambientale ed economica?}}{\textbf{Costi computazionali enormi}}
		\end{minipage}
		}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Futuro della Language AI}
	{\small
		\onslide<1->
		\framesubtitle{Verso l'AGI e oltre}
		\begin{itemize}[leftmargin=10pt,align=right]
			\onslide<2->\item[\alert{\faHandORight}] \alert{Modelli multimodali:} integrazione di testo, immagini, audio, video
			\onslide<3->\item[\alert{\faHandORight}] \alert{Agenti autonomi:} LLM che possono interagire con il mondo
			\onslide<4->\item[\alert{\faHandORight}] \alert{Efficienza:} modelli più piccoli ma più capaci
			\onslide<5->\item[\alert{\faHandORight}] \alert{Specializzazione:} LLM per domini specifici (medicina, legge, scienza)
		\end{itemize}
		\vspace*{.5cm}
		\begin{center}
			\alert{\Large Il viaggio da ELIZA a GPT-4 ha richiesto 58 anni.\\
			Cosa ci riserveranno i prossimi 10?}
		\end{center}
	}
\end{frame}
%
\begin{frame}[t,fragile] \frametitle{Riassunto: L'Evoluzione della Language AI}
	{\scriptsize
		\framesubtitle{Dal pattern matching all'intelligenza artificiale}
		\begin{center}
		{\tiny
		\begin{table}
			\setlength{\tabcolsep}{4pt}
			\renewcommand{\arraystretch}{1.3}
			\centering
			\begin{tabular}{p{1.8cm}p{2.2cm}p{2cm}p{2.5cm}p{2cm}}
				\toprule
				\textbf{Era} & \textbf{Modello} & \textbf{Anno} & \textbf{Approccio} & \textbf{Capacità}\\
				\midrule
				\alert{Regole} & ELIZA & 1966 & Pattern matching & Conversazione basilare\\
				\alert{Statistico} & Bag-of-Words & 1950-2000 & Conteggio parole & Classificazione testi\\
				\alert{Embeddings} & Word2Vec & 2013 & Reti neurali & Semantica delle parole\\
				\alert{Attenzione} & Transformer & 2017 & Self-attention & Comprensione contesto\\
				\alert{Pre-training} & BERT/GPT & 2018 & Transfer learning & Task multipli\\
				\alert{Scaling} & GPT-3/4 & 2020-2023 & Modelli enormi & Capacità emergenti\\
				\bottomrule
			\end{tabular}
		\end{table}
		}
		\end{center}
		\vspace*{.3cm}
		\begin{itemize}[leftmargin=10pt,align=right]
			\item[\alert{\faHandORight}] \alert{Lezione principale:} la scala (dati + parametri + compute) porta a capacità emergenti
			\item[\alert{\faHandORight}] \alert{Trend futuro:} efficienza, multimodalità, specializzazione
			\item[\alert{\faHandORight}] \alert{Impatto:} trasformazione di ogni settore che utilizza il linguaggio
		\end{itemize}
	}
\end{frame}